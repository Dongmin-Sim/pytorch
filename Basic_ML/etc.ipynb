{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "edwith의 부스트 코스 : \"파이토치로 시작하는 딥러닝 기초\" 를 바탕으로 작성되었습니다.  \n",
    "https://www.boostcourse.org/ai214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips\n",
    "- Maximum Likelihood Estimation\n",
    "- Optimization via Gradient Descent\n",
    "- Overfitting and Regrularization\n",
    "- Training and Test Dataset \n",
    "- Learning Rate\n",
    "- Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation (MLE)\n",
    "최대 가능도 추정, 최대 우도 추정,   \n",
    "\n",
    "예를 들어 동전을 던진다고 했을때, 동전의 앞면이 나올지(class 1), 동전의 뒷면이 나올지(class 2) 에 대한 확률 분포가 존재할 것. 이것을 ML을 통해서 예측하고자 하는 것이 목표,  \n",
    "그렇다면 예측해야하는 값이 2가지임, class1, class2 가 각각 나올 확률, 딱 2가지가 필요한데 이는 이항분포에 해당됨.(Bernoulli Distribution)  \n",
    "즉 0, 1의 binary classification을 수행하면 됨. 베르누이 분포를 여러번 하면 binomial distribution 이 됨.  \n",
    "\n",
    "실제로 동전을 100번 던졌을 때, class1이 나올 횟수가 30 번이라고 하면 binomial distribution 공식은 아래와 같음.  \n",
    "$$\\begin{aligned}\n",
    "   P(K=k)&=\\binom{n}{k} \\theta^{k}(1-\\theta)^{n-k} \\\\\n",
    "   &=\\dfrac{n!}{k!(n-k)!} \\cdot \\theta^{k}(1-\\theta)^{n-k}\n",
    "\\end{aligned}\n",
    "$$\n",
    "공식의 n = 100번 던졌을 때, k = 30 class1 이 나온 횟수라는 관찰값  \n",
    "구해야하는, 알고싶은 것이 $\\theta$임, 동전의 확률분포를 결정하는 파라미터 값이 되는 것.  \n",
    "위 공식에 따라서 $\\theta$ 에 대한 공식$f(\\theta)$을 만들어 낼 수 있음.  \n",
    "\n",
    "어떠한 $f(\\theta)$의 결과값이 Likelihood 인데, 이 우도 값이 가장 커지는 $\\theta$ 값이 바로 MLE 임.  \n",
    "이것이 관찰한 데이터를 가장 잘 설명하는 어떠한 확률 분포 함수의 파라미터를 찾아내는 과정이라고 할 수 있음.  \n",
    "\n",
    "\n",
    "## Optimization via Gradient Descent\n",
    "\n",
    "그렇다면 그 MLE 를 어떻게 찾을 것인가?  \n",
    "$f(\\theta)$ 공식을 그래프로 나타냈을 때, 기울기를 이용하여 구할 수 있음. 이는 likelihood 가 최대화되는 지점을 찾아야 하기 때문에 gradient ascent를 수행하게 됨.   \n",
    "\n",
    "이전까지 수행해왔던 gradient Descent와 ascent 는 다르지 않음, 최대를 찾는 것과 최소를 찾는 것. 반대로 수행하주면 됨.   \n",
    "그렇다면 구하고자 하는 $\\theta$ 는 어떤 손실함수의 $\\theta$ 값과 데이터가 주어졌을 때, 손실함수의 값을 $\\theta$에 대해서 미분한 것에 lr을 곱한 것을 $\\theta$에서 빼주게 되는 식으로 업데이트가 됨.  \n",
    "$$\\theta = \\theta - \\alpha \\bigtriangledown_{\\theta}L(x;\\theta)$$\n",
    "이것이 gradient descent 이고, 그렇게 되면 최소(local minima)가 되는 지점을 찾을 수 있게 됨. gradient ascent는 반대로 최대(local maxima)가 되는 지점을 찾을 수 있음.  \n",
    "\n",
    "이렇게 하면 주어진 데이터를 가장 잘 설명해줄 수 있는 파라미터 $\\theta$를 \"gradient descent/ascent\"라는 최적화를 통해서 찾을 수 있게 됨.  \n",
    "\n",
    "## Overfitting and Regrularization\n",
    "그러나 MLE 에는 overfitting 이 따르게 되는데 overgitting이란, 주어진 데이터에 대해서 과도하게 fitting 되어버린 케이스를 말함.     \n",
    "그래서 MLE 를 찾는 과정(주어진 데이터를 잘 설명하는 확률 분포 함수)에서는 필연적으로 overfitting이 일어날 수 밖에 없음.  \n",
    "그렇게 되면 다른 케이스에 대해서는 사용할 수 없게 되어버림(범용성 하락) 그래서 이러한 overfitting을 최소화 하는 것이 매우 중요함.  \n",
    "\n",
    "데이터를 훈련 set, 테스트 set, 을 특정 비율대로 나눠서 훈련한 후 얼마나 잘 학습되었는지 검증하는 방법을 사용할 수 있음.  \n",
    "보통 training set 은 0.8, test set 은 0.1~0.2 정도, development(validation)set은 없거나 0.1 정도 사용함.  \n",
    "그래서 training set 에 MLE 를 통해 과도하게 학습이 되었다면 test set 에 대하서는 좋은 성능을 내지 못할 것임. 이를 통해서 overfitting이 되었는지 안되었는지 판단할 수 있음.  \n",
    "\n",
    "test set 에 대해서도 overfitting 이 되는 경우가 발생할 수 있음. 이를 방지하기 위해 development(validation)set을 두는데, training set에 대하여 훈련을 하고 test set으로 검증을 하여, test set에서 가장 좋은 성능을 가진 모델을 선택하도록 만들게 된다면 결국에 training, test set 모두에 대해서 overfitting 되는 현상이 발생할 수 있음. 따라서 training 으로 훈련, dev(valid) 로 검증, 후 test 하면 훨씬 더 정확한 성능을 얻을 수 있을 것임.  \n",
    "\n",
    "epoch을 거칠 수록 training loss(training set 에 대한 loss값) 는 떨어지게 되는데 만약 validation loss 값과의 차이가 점점 커지게 된다면 이는 overfitting이 일어나고 있다 라고 판단할 수 있을 것임.  \n",
    "validation loss 값이 더 커지지 않는 선에서 훈련을 중지하거나, 가장 적은 loss의 epoch 에서의 모델을 선택할 수 도 있음. 이러한 overfitting을 줄이기 위한 많은 방법들이 있는데, (연구 진행)  \n",
    "\n",
    "1. 데이터의 양을 늘린다. - 데이터가 적으면 편향될 가능성이 높고, 데이터가 많을 수록 실제와 가까운 데이터 set을 얻을 수 있기 때문\n",
    "2. feature를 적게 사용. - feature 란 어떤 분포를 잘 설명하는 특징을 말함. \n",
    "3. Regrularization \n",
    "- Early stopping : validation loss 가 더 이상 낮아지지 않을 때 중단.\n",
    "- weight decay : 파라미터수를 제한 방법. \n",
    "- neural network의 사이즈를 줄임. \n",
    "- **dropout**\n",
    "- **batch Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Approach to TRAIN DNN\n",
    "1. Make a neural network architecture.  \n",
    "    예시) 1d 벡터에 10개의 feature(10dim) 를 가진 데이터가 주어질때, 이것을 5 class classification 하려고 한다면, 5개의 unit 을 가진 softmax layer를 구성.  \n",
    "    \n",
    "2. Train and check that model is over-fitted. \n",
    "    - if it is not, increase the model size (deeper and wider)\n",
    "    - if it is, add regularization, such as drop-out, batch-normalization\n",
    "3. Repeat from step-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fed107d16b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5], \n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [2, 2, 2, 1, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape :  torch.Size([8, 3]) \n",
      "y_train shape :  torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = torch.FloatTensor(x_data), torch.LongTensor(y_data)\n",
    "print('x_train shape : ', x_train.shape, '\\ny_train shape : ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape :  torch.Size([3, 3]) \n",
      "y_test shape :  torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.FloatTensor([[2, 1, 1], [3, 1, 2], [3, 3, 4]])\n",
    "y_test = torch.LongTensor([2, 2, 2])\n",
    "\n",
    "print('x_test shape : ', x_test.shape, '\\ny_test shape : ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 3)  # 1개의 linear layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # input data 를 linear layer 에 입력, 통과시킨 결과를 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, x_train, y_train, epochs):\n",
    "    nb_epochs = epochs\n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        # cost 계산\n",
    "        cost = F.cross_entropy(prediction, y_train)\n",
    "        \n",
    "        # cost 로 H(x) 계산\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, optimizer, x_test, y_test):\n",
    "    y_pred = model(x_test)\n",
    "    pred_classes = y_pred.max(1)[1]\n",
    "    \n",
    "    correct_count = (pred_classes == y_test).sum().item()\n",
    "    cost = F.cross_entropy(y_pred, y_test)\n",
    "    \n",
    "    print('Accuracy {}%, Cost {:.6f}'.format(correct_count / len(y_test) * 100, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost 0.983424\n",
      "Epoch    1/20 Cost 0.977591\n",
      "Epoch    2/20 Cost 0.971864\n",
      "Epoch    3/20 Cost 0.966240\n",
      "Epoch    4/20 Cost 0.960718\n",
      "Epoch    5/20 Cost 0.955295\n",
      "Epoch    6/20 Cost 0.949968\n",
      "Epoch    7/20 Cost 0.944736\n",
      "Epoch    8/20 Cost 0.939596\n",
      "Epoch    9/20 Cost 0.934546\n",
      "Epoch   10/20 Cost 0.929585\n",
      "Epoch   11/20 Cost 0.924709\n",
      "Epoch   12/20 Cost 0.919918\n",
      "Epoch   13/20 Cost 0.915210\n",
      "Epoch   14/20 Cost 0.910582\n",
      "Epoch   15/20 Cost 0.906033\n",
      "Epoch   16/20 Cost 0.901561\n",
      "Epoch   17/20 Cost 0.897164\n",
      "Epoch   18/20 Cost 0.892841\n",
      "Epoch   19/20 Cost 0.888590\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 66.66666666666666% Cost 0.791733\n"
     ]
    }
   ],
   "source": [
    "test(model, optimizer, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "learning rate 가 너무 크면 diverge 하면서 cost 가 점점 늘어남(overshooting)\n",
    "learning rate 조절을 통해서 학습 속도를 조절하여 학습을 진행할 수 있음.  \n",
    "\n",
    "learning rate 가 너무 크면 gradient descent 하는 step 이 커지므로써, loss 값의 최소로 다가가는 것이 아닌, 발산해버리는, 커져버리는 현상이 생김. == overshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost 1.280268\n",
      "Epoch    1/20 Cost 8.759867\n",
      "Epoch    2/20 Cost 13.274830\n",
      "Epoch    3/20 Cost 12.300616\n",
      "Epoch    4/20 Cost 10.008135\n",
      "Epoch    5/20 Cost 19.936989\n",
      "Epoch    6/20 Cost 2.380140\n",
      "Epoch    7/20 Cost 14.007809\n",
      "Epoch    8/20 Cost 14.337420\n",
      "Epoch    9/20 Cost 6.581949\n",
      "Epoch   10/20 Cost 15.990554\n",
      "Epoch   11/20 Cost 4.531405\n",
      "Epoch   12/20 Cost 11.478882\n",
      "Epoch   13/20 Cost 12.421989\n",
      "Epoch   14/20 Cost 7.942746\n",
      "Epoch   15/20 Cost 18.664379\n",
      "Epoch   16/20 Cost 2.209504\n",
      "Epoch   17/20 Cost 12.260416\n",
      "Epoch   18/20 Cost 10.136712\n",
      "Epoch   19/20 Cost 8.135773\n"
     ]
    }
   ],
   "source": [
    "model = SoftmaxClassifierModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "train(model, optimizer, x_train, y_train, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반대로 learning rate가 너무 작아도 문제가 생김. gradient descent 하는 step 이 작아지면, loss 값이 최소가 되는 지점으로 가는데 매우 더디게 된다는 점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost 3.187324\n",
      "Epoch    1/20 Cost 3.187324\n",
      "Epoch    2/20 Cost 3.187324\n",
      "Epoch    3/20 Cost 3.187324\n",
      "Epoch    4/20 Cost 3.187324\n",
      "Epoch    5/20 Cost 3.187324\n",
      "Epoch    6/20 Cost 3.187324\n",
      "Epoch    7/20 Cost 3.187324\n",
      "Epoch    8/20 Cost 3.187324\n",
      "Epoch    9/20 Cost 3.187324\n",
      "Epoch   10/20 Cost 3.187324\n",
      "Epoch   11/20 Cost 3.187324\n",
      "Epoch   12/20 Cost 3.187324\n",
      "Epoch   13/20 Cost 3.187324\n",
      "Epoch   14/20 Cost 3.187324\n",
      "Epoch   15/20 Cost 3.187324\n",
      "Epoch   16/20 Cost 3.187324\n",
      "Epoch   17/20 Cost 3.187324\n",
      "Epoch   18/20 Cost 3.187324\n",
      "Epoch   19/20 Cost 3.187324\n"
     ]
    }
   ],
   "source": [
    "model = SoftmaxClassifierModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-10)\n",
    "train(model, optimizer, x_train, y_train, 20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "그래서 데이터와 모델에 맞는 적절한, 최적의 learning rate를 찾는 것이 중요함.   \n",
    "발산한다면 learning rate 를 작게, 학습이 진행되지 않는 다면, 학습이 느리다면 learning rate 를 늘리면서 찾아나가면 됨. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "데이터 전처리, gradient descent 를 통해서 최적화를 수행하기 때문에 전처리를 통해 데이터를 미리 학습하기 쉽도록 바꿔주는 것이 굉장히 중요함.  \n",
    "아래와 같은 데이터가 있을때, regression 문제일 때, data preprocessing 을 해주게 되면 훨씬 더 학습이 수월해짐.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                            [93, 88, 93],\n",
    "                            [89, 91, 90],\n",
    "                            [96, 98, 100],\n",
    "                            [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* standardization \n",
    "$$x'_j = \\dfrac{x_j - \\mu_j}{\\sigma_j}$$\n",
    "\n",
    "여기서 $\\sigma$ 는 stardard deviation, $\\mu$는 평균값  \n",
    "정규분포로 만들어주어 normalization 을 수행함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = x_train.mean(dim=0)\n",
    "sigma = x_train.std(dim=0)\n",
    "norm_x_train = (x_train - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0674, -0.3758, -0.8398],\n",
      "        [ 0.7418,  0.2778,  0.5863],\n",
      "        [ 0.3799,  0.5229,  0.3486],\n",
      "        [ 1.0132,  1.0948,  1.1409],\n",
      "        [-1.0674, -1.5197, -1.2360]])\n"
     ]
    }
   ],
   "source": [
    "print(norm_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)  # 1개의 linear layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # input data 를 linear layer 에 입력, 통과시킨 결과를 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, x_train, y_train, epochs):\n",
    "    nb_epochs = epochs\n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        # cost 로 H(x) 계산\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost 29729.949219\n",
      "Epoch    1/20 Cost 18889.082031\n",
      "Epoch    2/20 Cost 12048.976562\n",
      "Epoch    3/20 Cost 7699.843750\n",
      "Epoch    4/20 Cost 4924.700195\n",
      "Epoch    5/20 Cost 3151.020264\n",
      "Epoch    6/20 Cost 2016.562866\n",
      "Epoch    7/20 Cost 1290.709106\n",
      "Epoch    8/20 Cost 826.216003\n",
      "Epoch    9/20 Cost 528.952271\n",
      "Epoch   10/20 Cost 338.703308\n",
      "Epoch   11/20 Cost 216.940063\n",
      "Epoch   12/20 Cost 139.006989\n",
      "Epoch   13/20 Cost 89.125130\n",
      "Epoch   14/20 Cost 57.196083\n",
      "Epoch   15/20 Cost 36.757317\n",
      "Epoch   16/20 Cost 23.672049\n",
      "Epoch   17/20 Cost 15.293401\n",
      "Epoch   18/20 Cost 9.927165\n",
      "Epoch   19/20 Cost 6.488903\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, norm_x_train, y_train, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리를 수행하지 않았다면, 최적화에서 어려움을 겪을 수 있음.  \n",
    "만약 y_train 값이 2차원의 2개의 element를 가진 벡터인 상황에서 두 element 간의 값의 크기차이가 크다면, neural network 는 큰 값에 대해서 가중치를 더 많이 두고, 큰 값을 학습하는데 비중이 더 커질 것임.  \n",
    "하지만 이것을 전처리를 통해 똑같은 값으로 변환해준다면 neural network 는 두 값에 대해서 동등한 가중치를 두고 학습을 할 수 있을 것.  \n",
    "이와 같이 데이터의 분석을 통해서 데이터의 형질, 형태를 파악하고 전처리를 해주는 과정은 매우 중요함.  \n",
    "결국 maximum likelihood 를 gradient descent 를 통해서 최적화를 해주게 되는데 이를 못하면 최적의 파라미터를 찾을 수 없기 때문임. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
