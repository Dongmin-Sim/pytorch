{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "edwith의 부스트 코스 : \"파이토치로 시작하는 딥러닝 기초\" 를 바탕으로 작성되었습니다.  \n",
    "https://www.boostcourse.org/ai214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "- Data definition\n",
    "- Hypothesis\n",
    "- Compute loss\n",
    "- Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data definition\n",
    "본 장에서 model 하려고 하는 것은 공부한 시간과 성적간의 관계임.\n",
    "\n",
    "실제로 주어진 데이터, 공부한 시간과 성적은 training data 라고 함.  \n",
    "학습이 끝난 후 model 을 판별하기 위한 data를 test data 라고 함.\n",
    "\n",
    "model 을 학습시키기 위한 data 는 `torch.tensor` 의 형태를 띄고 입력과 출력을 각기 다른 tensor 에 저장함.  \n",
    "입력 : x_train (공부한 시간)  \n",
    "출력 : y_train (성적)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "$$y = Wx + b$$\n",
    "Linear regression 은 학습 data 와 가장 잘 맞는 하나의 직선을 찾는 일임.  \n",
    "이러한 직선 y 는 위의 식처럼 나타낼 수 있음.   \n",
    "여기서 x애 곱해지는 W(weight), b(bias)는 각각 가중치, 편향이라는 뜻을 가짐. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 W, b 를 정의 해줌. 0으로 초기화를 시켜준 후, W와 b를 학습시키기 위해 requires_grad 라는 옵션값은 True로 지정함. \n",
    "\n",
    "```python\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "hypothesis = x_train * W + b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute loss\n",
    "학습을 하려면 얼마나 정답과 가까운지 알아야한. 이 숫자를 cost loss 라고 함.  \n",
    "Linear regression 에서는 이 cost, loss 를 계산하기 위한 함수로 MSE, mean Squared Error 이라고 함.  \n",
    "$$cost(W, b) = \\frac{1}{m} \\sum^{m}_{i=1} \\left(H(x^{(i)}) - y^{(i)} \\right)^{2}$$ \n",
    "\n",
    "MSE로 계산된 loss 값은 단순히 예측값과 실제 Training dataset 의 y 값의 차이를 제곱해서 평균한 값임.  \n",
    "이를 PyTorch로 다음과 같이 구현할 수 있음\n",
    "\n",
    "```python\n",
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "loss 값을 줄이기 위해 경사하강법을 사용하는데 SGD stochastic Gradient Descent 방법을 사용함.  \n",
    "`torch.optim` 라이브러리에 해당 경사하강법 함수가 존재. 이를 optimizer 로 설정해줌. \n",
    "\n",
    "```python\n",
    "optimizer = optim.SGD([w, b], lr=0.01)\n",
    "```\n",
    "학습시킬 값을 리스트 형태로 넣어주고, learning_rate 값을 지정\n",
    "\n",
    "아래의 코드 3줄은 gradient 초기화, gradient 계산, step() 으로 업데이트하는 코드로 실질적으로 학습을 시켜주는 코드임\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 완료 \n",
      "W :  tensor([1.9708], requires_grad=True) \n",
      "b :  tensor([0.0664], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "epoch = 1000\n",
    "for _ in range(1, epoch + 1):\n",
    "    hypothesis = x_train * W + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print('학습 완료', '\\nW : ', W, '\\nb : ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 반복적으로 학습을 진행하면 W와 b가 각각 하나의 숫자로 수렴하게 됨. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
