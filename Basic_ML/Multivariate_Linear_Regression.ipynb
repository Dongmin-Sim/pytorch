{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "edwith의 부스트 코스 : \"파이토치로 시작하는 딥러닝 기초\" 를 바탕으로 작성되었습니다.  \n",
    "https://www.boostcourse.org/ai214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression\n",
    "- Multivariate Linear Regression 이론\n",
    "- Naive Data Representation\n",
    "- Matrix Data Representation\n",
    "- Multivariate Linear Regression \n",
    "- nn.Module\n",
    "- F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression 이론\n",
    "쪽지 시험별 성적이 각각 다른 학생의 기말고사 점수를 예측하는 모델\n",
    "\n",
    "### dataset \n",
    "다섯명의 학생의 3번의 쪽지시험 점수가 입력으로, 기말고사 점수가 출력으로 주어짐. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90], \n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Function \n",
    "simple linear regression 처럼 동일하게 $W * x + b$ 의 형태를 취하는데 x 를 1 * 1 벡터로 표현했다면  \n",
    "현재의 x 값은 하나의 값이 아니라 여러값이기 때문에 3 * 1 벡터로 표현할 수 있을 것임.  \n",
    "$$H(x) = w_1x_1 + w_2x_2 + w_3x_3 + b$$\n",
    "\n",
    "위의 수식처럼 표현이 가능한데, 이보다 더 많은 양의 x를 가지고 있다면 이를 코드로 구현하는 것은 불가능에 가까움.  \n",
    "그렇기 때문에 PyTorch 에서 제공하는 `matmul()` 함수를 이용함. matmul = matrix multipication의 줄임말.  \n",
    "\n",
    "### Cost function : MSE \n",
    "simple linear regression 과 마찬가지로 MSE를 사용하며 계산방식 역시 동일\n",
    "\n",
    "### Gradient Descent \n",
    "학습방식 역시 동일함. optimizer 를 정의한 후 cost를 구할때마다 optimizer의 gradient 에 저장한 후 gradient descent 를 시행함. \n",
    "\n",
    "### Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    1/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost : 29661.800781\n",
      "Epoch :    2/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost : 9298.520508\n",
      "Epoch :    3/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost : 2915.712646\n",
      "Epoch :    4/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost : 915.040527\n",
      "Epoch :    5/20 hypothesis: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]) Cost : 287.936005\n",
      "Epoch :    6/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost : 91.371017\n",
      "Epoch :    7/20 hypothesis: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]) Cost : 29.758139\n",
      "Epoch :    8/20 hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]) Cost : 10.445305\n",
      "Epoch :    9/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost : 4.391228\n",
      "Epoch :   10/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost : 2.493135\n",
      "Epoch :   11/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost : 1.897688\n",
      "Epoch :   12/20 hypothesis: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]) Cost : 1.710541\n",
      "Epoch :   13/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost : 1.651413\n",
      "Epoch :   14/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost : 1.632387\n",
      "Epoch :   15/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost : 1.625923\n",
      "Epoch :   16/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost : 1.623412\n",
      "Epoch :   17/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost : 1.622141\n",
      "Epoch :   18/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost : 1.621253\n",
      "Epoch :   19/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]) Cost : 1.620500\n",
      "Epoch :   20/20 hypothesis: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]) Cost : 1.619770\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    hypothesis = x_train.matmul(W) + b  # matmul 로 \n",
    "    \n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    print('Epoch : {:4d}/{} hypothesis: {} Cost : {:.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module\n",
    "nn.Modul 을 사용하여 클래스를 만들면 모델을 보다 쉽게 만들 수 있음. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class Multivariate_Linear_RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = Multivariate_Linear_RegressionModel()\n",
    "    \n",
    "hypothesis = model(x_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module 를 상속해서 모델을 생성,  \n",
    "nn.Linear(3, 1) 입력차원과 출력차원을 입력  \n",
    "Hypothesis 계산은 forward 함수에서 진행되며, \n",
    "gradient 계산은 PyTroch에서 자동으로 진행해줌.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.mse_loss \n",
    "PyTorch 에서는 다양한 cost function 을 제공함.  \n",
    "PyTorch 의 cost function 을 사용하면 다음 cost function 을 바꿀때 좀 더 편리하고, cost function 을 계산하면서 생기는 버그가 없어 디버깅이 쉽다는 장점이 있음.  \n",
    "mse는 torch.nn.functional 에 존재하는 mse_loss 를 사용하면 됨.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch.nn.functional as F\n",
    "\n",
    "cost = F.mse_loss(prediction, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    1/20 hypothesis: tensor([-9.3751, -4.1805, -7.8001, -8.0960, -1.7647]) Cost : 31884.699219\n",
      "Epoch :    2/20 hypothesis: tensor([60.3576, 79.6319, 74.7822, 81.8341, 62.1629]) Cost : 9995.871094\n",
      "Epoch :    3/20 hypothesis: tensor([ 99.3986, 126.5553, 121.0172, 132.1826,  97.9533]) Cost : 3134.886475\n",
      "Epoch :    4/20 hypothesis: tensor([121.2566, 152.8257, 146.9025, 160.3710, 117.9909]) Cost : 984.331177\n",
      "Epoch :    5/20 hypothesis: tensor([133.4942, 167.5334, 161.3948, 176.1526, 129.2090]) Cost : 310.245728\n",
      "Epoch :    6/20 hypothesis: tensor([140.3459, 175.7675, 169.5086, 184.9883, 135.4893]) Cost : 98.954575\n",
      "Epoch :    7/20 hypothesis: tensor([144.1822, 180.3773, 174.0513, 189.9351, 139.0052]) Cost : 32.725243\n",
      "Epoch :    8/20 hypothesis: tensor([146.3302, 182.9580, 176.5946, 192.7047, 140.9734]) Cost : 11.965099\n",
      "Epoch :    9/20 hypothesis: tensor([147.5331, 184.4027, 178.0186, 194.2553, 142.0752]) Cost : 5.457086\n",
      "Epoch :   10/20 hypothesis: tensor([148.2068, 185.2113, 178.8160, 195.1235, 142.6917]) Cost : 3.416355\n",
      "Epoch :   11/20 hypothesis: tensor([148.5842, 185.6639, 179.2625, 195.6096, 143.0367]) Cost : 2.775859\n",
      "Epoch :   12/20 hypothesis: tensor([148.7958, 185.9171, 179.5125, 195.8818, 143.2296]) Cost : 2.574324\n",
      "Epoch :   13/20 hypothesis: tensor([148.9145, 186.0587, 179.6526, 196.0343, 143.3374]) Cost : 2.510326\n",
      "Epoch :   14/20 hypothesis: tensor([148.9812, 186.1377, 179.7311, 196.1197, 143.3976]) Cost : 2.489473\n",
      "Epoch :   15/20 hypothesis: tensor([149.0188, 186.1819, 179.7751, 196.1675, 143.4310]) Cost : 2.482130\n",
      "Epoch :   16/20 hypothesis: tensor([149.0401, 186.2064, 179.7998, 196.1944, 143.4495]) Cost : 2.479028\n",
      "Epoch :   17/20 hypothesis: tensor([149.0523, 186.2199, 179.8138, 196.2095, 143.4596]) Cost : 2.477247\n",
      "Epoch :   18/20 hypothesis: tensor([149.0594, 186.2273, 179.8216, 196.2180, 143.4651]) Cost : 2.475876\n",
      "Epoch :   19/20 hypothesis: tensor([149.0636, 186.2313, 179.8261, 196.2228, 143.4679]) Cost : 2.474629\n",
      "Epoch :   20/20 hypothesis: tensor([149.0662, 186.2334, 179.8287, 196.2255, 143.4693]) Cost : 2.473456\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Multivariate_Linear_RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = Multivariate_Linear_RegressionModel()\n",
    "    \n",
    "\n",
    "# W = torch.zeros((3, 1), requires_grad=True)\n",
    "# b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    # H(x) 계산\n",
    "#     hypothesis = x_train.matmul(W) + b\n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    # Cost 계산\n",
    "#     cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    cost = F.mse_loss(hypothesis, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch : {:4d}/{} hypothesis: {} Cost : {:.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
