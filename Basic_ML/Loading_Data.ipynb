{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "edwith의 부스트 코스 : \"파이토치로 시작하는 딥러닝 기초\" 를 바탕으로 작성되었습니다.  \n",
    "https://www.boostcourse.org/ai214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "- \"minibatch\" Gradient Descent\n",
    "- PyTorch Dataset and DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매우 간단한 모델을 학습시킬때는 데이터의 양이 적어도 괜찮지만,  \n",
    "복잡한 머신러닝 모델들을 학습하려면 엄청난 양의 데이터가 필요함. 현재 존재하는 dataset은 굉장히 많은 데이터들을 가지고 있는데  \n",
    "비전 쪽 분야에서 가장 널리 알려진 IMAGENET 같은 dataset 은 천만장이 넘는 이미지가 데이터로 존재함.  \n",
    "데이터가 많으면 많을 수록 모델이 많은 데이터를 학습하면서 더 좋은 예측을 할 수 있기 때문임.   \n",
    "하지만 데이터가 너무 많다면 데이터의 각각의 gradient descent 를 하기 위해 cost 를 구해야하는데 이를 한번에 학습시키기는 불가능함.  \n",
    "\n",
    "일부분의 데이터를 학습시키는 방식이 Minibatch Gradient Descent 임.  \n",
    "전체 데이터를 작은 Minibatch 라는 작은 양으로 균일하게 나누어서 Minibatch 하나하나씩 학습하는 방법임.  \n",
    "이렇게 하면 모든 데이터의 cost 를 다 계산한 후 gradient descent를 하는 것이 아니라, 각 minibatch 에 있는 데이터의 cost만 계산한 후 gradient descent를 하기 때문에  \n",
    "한 번 업데이트마다 계산한 cost 의 양은 줄어들고 그에 따른 업데이트 주기가 빨라지게 됨.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Dataset\n",
    "PyTorch Dataset 은 PyTorch에서 제공하는 모듈로 이 모듈을 상속해 새로운 클래스를 만드는 방식으로 원하는 dataset을 지정할 수 있게 됨.  \n",
    "이렇게 custom으로 dataset 클래스를 만들게 되면 2가지의 magic 메소드를 구현해야하는데  \n",
    "1. dataset의 총 데이터 개수를 반환하는 `__len__()`이라는 메서드\n",
    "2. 인덱스를 입력받았을때, 인덱스에 해당하는 데이터를 반환하는 `__getitem__()`이라는 메서드  \n",
    "가 필요함.  `len`에서는 dataset의 총 데이터 개수를 반환하고, `getitem` 에서는 인덱스를 받아 그에 맞는 입력과 출력 데이터를 찾은 후 torch.tensor 형태로 바꾸어서 반환함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = [[73, 80, 75],\n",
    "                       [93, 88, 93],\n",
    "                       [89, 91, 90], \n",
    "                       [96, 98, 100],\n",
    "                       [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y\n",
    "    \n",
    "dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "dataloader 의 인스턴스를 만들려면 2개를 지정해야하는데,  \n",
    "1. 위에서 생성한 dataset 이고,  \n",
    "2. 각 minibatch 의 크기임.  \n",
    "통상적으로 minibatch의 크기는 2의 제곱수로 설정함.   \n",
    "한가지 옵션을 추가해 shuffle=True 로 매번 데이터가 학습되는 순서를 섞어주는 옵션임.  \n",
    "이를 설정함으로써 모델이 dataset의 순서를 외우지 못하게 방지할 수 있으므로, 주로 권장되는 옵션임.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset,\n",
    "                       batch_size=2,\n",
    "                       shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code\n",
    "\n",
    "epoch 안에 minibatch 를 위한 for문 하나가 더 추가 되었는데 이때 enumerate(dataloader) 를 사용함.  \n",
    "이것은 minibatch 의 인덱스, 몇 번째 minibatch 인지, 와 minibatch_size에 의해 나누어진 데이터를 줌.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    1/20 batch: 1/3 Cost : 23414.621094\n",
      "Epoch :    1/20 batch: 2/3 Cost : 3205.951416\n",
      "Epoch :    1/20 batch: 3/3 Cost : 2376.525391\n",
      "Epoch :    2/20 batch: 1/3 Cost : 542.302368\n",
      "Epoch :    2/20 batch: 2/3 Cost : 123.560890\n",
      "Epoch :    2/20 batch: 3/3 Cost : 27.622614\n",
      "Epoch :    3/20 batch: 1/3 Cost : 18.889156\n",
      "Epoch :    3/20 batch: 2/3 Cost : 2.911759\n",
      "Epoch :    3/20 batch: 3/3 Cost : 7.161684\n",
      "Epoch :    4/20 batch: 1/3 Cost : 0.999741\n",
      "Epoch :    4/20 batch: 2/3 Cost : 0.891147\n",
      "Epoch :    4/20 batch: 3/3 Cost : 0.625923\n",
      "Epoch :    5/20 batch: 1/3 Cost : 1.152383\n",
      "Epoch :    5/20 batch: 2/3 Cost : 0.857426\n",
      "Epoch :    5/20 batch: 3/3 Cost : 0.055068\n",
      "Epoch :    6/20 batch: 1/3 Cost : 1.027473\n",
      "Epoch :    6/20 batch: 2/3 Cost : 0.338722\n",
      "Epoch :    6/20 batch: 3/3 Cost : 1.558497\n",
      "Epoch :    7/20 batch: 1/3 Cost : 1.033177\n",
      "Epoch :    7/20 batch: 2/3 Cost : 0.374559\n",
      "Epoch :    7/20 batch: 3/3 Cost : 1.304439\n",
      "Epoch :    8/20 batch: 1/3 Cost : 0.103729\n",
      "Epoch :    8/20 batch: 2/3 Cost : 2.023771\n",
      "Epoch :    8/20 batch: 3/3 Cost : 1.138329\n",
      "Epoch :    9/20 batch: 1/3 Cost : 1.791167\n",
      "Epoch :    9/20 batch: 2/3 Cost : 1.166696\n",
      "Epoch :    9/20 batch: 3/3 Cost : 0.003134\n",
      "Epoch :   10/20 batch: 1/3 Cost : 1.093744\n",
      "Epoch :   10/20 batch: 2/3 Cost : 0.835765\n",
      "Epoch :   10/20 batch: 3/3 Cost : 0.061444\n",
      "Epoch :   11/20 batch: 1/3 Cost : 1.096350\n",
      "Epoch :   11/20 batch: 2/3 Cost : 0.537287\n",
      "Epoch :   11/20 batch: 3/3 Cost : 0.773281\n",
      "Epoch :   12/20 batch: 1/3 Cost : 0.816042\n",
      "Epoch :   12/20 batch: 2/3 Cost : 0.599491\n",
      "Epoch :   12/20 batch: 3/3 Cost : 2.269543\n",
      "Epoch :   13/20 batch: 1/3 Cost : 0.714106\n",
      "Epoch :   13/20 batch: 2/3 Cost : 0.528841\n",
      "Epoch :   13/20 batch: 3/3 Cost : 1.804639\n",
      "Epoch :   14/20 batch: 1/3 Cost : 0.766652\n",
      "Epoch :   14/20 batch: 2/3 Cost : 1.076904\n",
      "Epoch :   14/20 batch: 3/3 Cost : 0.150663\n",
      "Epoch :   15/20 batch: 1/3 Cost : 0.767518\n",
      "Epoch :   15/20 batch: 2/3 Cost : 0.709808\n",
      "Epoch :   15/20 batch: 3/3 Cost : 0.902436\n",
      "Epoch :   16/20 batch: 1/3 Cost : 1.072285\n",
      "Epoch :   16/20 batch: 2/3 Cost : 0.505211\n",
      "Epoch :   16/20 batch: 3/3 Cost : 1.904919\n",
      "Epoch :   17/20 batch: 1/3 Cost : 0.617278\n",
      "Epoch :   17/20 batch: 2/3 Cost : 0.823828\n",
      "Epoch :   17/20 batch: 3/3 Cost : 1.471141\n",
      "Epoch :   18/20 batch: 1/3 Cost : 0.910699\n",
      "Epoch :   18/20 batch: 2/3 Cost : 1.061752\n",
      "Epoch :   18/20 batch: 3/3 Cost : 0.870885\n",
      "Epoch :   19/20 batch: 1/3 Cost : 1.196957\n",
      "Epoch :   19/20 batch: 2/3 Cost : 0.702069\n",
      "Epoch :   19/20 batch: 3/3 Cost : 1.051645\n",
      "Epoch :   20/20 batch: 1/3 Cost : 1.074221\n",
      "Epoch :   20/20 batch: 2/3 Cost : 0.837805\n",
      "Epoch :   20/20 batch: 3/3 Cost : 0.168253\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Multivariate_Linear_RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = Multivariate_Linear_RegressionModel()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        \n",
    "        # H(x) 계산\n",
    "        hypothesis = model(x_train)\n",
    "\n",
    "        # Cost 계산\n",
    "        cost = F.mse_loss(hypothesis, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch : {:4d}/{} batch: {}/{} Cost : {:.6f}'.format(epoch, nb_epochs, batch_idx+1, len(dataloader), cost.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
