{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "edwith의 부스트 코스 : \"파이토치로 시작하는 딥러닝 기초\"를 바탕으로 작성되었습니다.  \n",
    "https://www.boostcourse.org/ai214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "- Logistic Regression\n",
    "- Computing Hypothesis\n",
    "- Computing Cost Function\n",
    "- Evaluation\n",
    "- Higher Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression 이론\n",
    "\n",
    "Hypothesis \n",
    "$$H(x) = \\dfrac{1}{1+e^{-W^{T}X}}$$\n",
    "\n",
    "Logistic Regression 은 Binary Classificaion 문제임.  \n",
    "예를 들어 m개의 샘플로 이루어진 d 라는 차원을 가진 X 데이터가 있을때, d 차원의 d사이즈의 벡터가 m개 만큼 존재할 때, **m 개의 0과1**로 이루어진 1d벡터(정답)를 구할 수 있도록하는 것이 Binary classification 의 문제가 됨.  \n",
    "d 차원의 d 사이즈의 1d 벡터가 주어졌을때, 이것이 0 또는 1 중에서 어느쪽에 가까운지 구하는 문제임. 이를 수식으로 표현하자면   \n",
    "\n",
    "$$\\begin{aligned}\n",
    "   H(X)&= P(x=1;W) \\\\\n",
    "   &=1 - P(x=0;W)\n",
    "\\end{aligned}$$ \n",
    "  \n",
    "위와 같이 표현할 수 있을 것임. 이를 Logisic Regression을 통해서 x가 1일 확률을 구해야 함. \n",
    "Logisic Regression의 모델 파라미터 W는 (d * 1) 크기의 weight를 갖게 됨.  \n",
    "따라서 X 데이터 (m * d)와 W (d * 1) 를 행렬곱을 해준다음 sigmoid 라는 함수를 통과하여 0~1 사이의 값에 근사하도록 하여 binary classification 문제를 해결할 수 있음.\n",
    "\n",
    "### sigmoid\n",
    "sigmoid 는 함수는 -무한대는 0, +무한대는 1에 가까운 값을 반환하는 함수  \n",
    "$$sigmoid(x) = \\dfrac{1}{1+e^{-X}}$$\n",
    "\n",
    "\n",
    "다시 위의 Logistic Regression의 hypothesis는 다음과 같이 표현 될 수 있다. \n",
    "$$\\begin{aligned}\n",
    "   H(X)&= \\dfrac{1}{1+e^{-W^{T}X}} \\\\\n",
    "   \\approx P(X=1)&=\\dfrac{1}{1+e^{-X W}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "구하고자 하는 H(x) 는 x가 1일 확률을 근사하는 것이므로 위처럼 표현이 될 수 있다.  \n",
    "수식에서 W X를 행렬곱을 해주기 때문에 행렬곱 수행후 나오게 되는 값의 shape 은 (m, 1) m개의 element 를 가진 1d 벡터가 되는것을 확인할 수 있음  \n",
    "즉, 구하고자 하는 **m 개의 0과1**로 이루어진 1d벡터(정답)인 (m * 1) 을 얻을 수 있게 된다는 뜻.\n",
    "\n",
    "### Cost\n",
    "cost 함수는 다음과 같이 구할 수 있음. \n",
    "$$cost(W) = -\\dfrac{1}{m} \\sum y log(H(x)) + (1 - y)(log(1-H(x))$$\n",
    "\n",
    "### Weight Update via Gradient Descent\n",
    "즉, W는 gradient descent 를 통해서 훈련이 가능함. 파라미터 W에 대해서 미분한 gradient 에 대해서 learning rate 를 곱하고, 그 값은 W에서 빼주는 방식으로 gradient descent를 진행하게 됨.  \n",
    "이렇게 되면 W 가 업데이트 되는 것이며, loss 를 최소화 하는 방향으로 학습이 되게 됨. \n",
    "$$\n",
    "W := W - \\alpha \\dfrac{\\partial}{\\partial W}cost(W)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa00873e3d0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random seed\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학생들의 강의수강 시간과, 연구시간이 주어질 때, 해당 강의를 수료할 수 있는지 없는지를 binary classification 하는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.size())\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Hypothesis\n",
    "\n",
    "$$\\begin{aligned}\n",
    "   H(X)&= \\dfrac{1}{1+e^{-W^{T}X}} \\\\\n",
    "   \\approx P(X=1)&=\\dfrac{1}{1+e^{-X W}} \\\\\n",
    "   &=\\dfrac{1}{1+e^{-(XW + b)}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e^1 equals :  tensor([2.7183])\n"
     ]
    }
   ],
   "source": [
    "print('e^1 equals : ', torch.exp(torch.FloatTensor([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<MulBackward0>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(hypothesis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch 에서는 `torch.sigmoid()` 함수를 제공하여 보다 쉽게 작성이 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e^1 equals :  tensor([0.7311])\n"
     ]
    }
   ],
   "source": [
    "print('e^1 equals : ', torch.sigmoid(torch.FloatTensor([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(hypothesis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Cost Function\n",
    "$$cost(W) = -\\dfrac{1}{m} \\sum y log(H(x)) + (1 - y)(log(1-H(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6931], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- (y_train[0] * torch.log(hypothesis[0]) + (1 - y_train[0]) * torch.log(1 - hypothesis[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931]], grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "losses = - (y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis))\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean(losses)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch 에서는 `F.binary_cross_entropy()` 를 제공하여 보다 쉽게 cost 함수를 구할 수 있음.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    0/1000 Cost : 0.693147\n",
      "Epoch :  100/1000 Cost : 0.134722\n",
      "Epoch :  200/1000 Cost : 0.080643\n",
      "Epoch :  300/1000 Cost : 0.057900\n",
      "Epoch :  400/1000 Cost : 0.045300\n",
      "Epoch :  500/1000 Cost : 0.037261\n",
      "Epoch :  600/1000 Cost : 0.031672\n",
      "Epoch :  700/1000 Cost : 0.027556\n",
      "Epoch :  800/1000 Cost : 0.024394\n",
      "Epoch :  900/1000 Cost : 0.021888\n",
      "Epoch : 1000/1000 Cost : 0.019852\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화 \n",
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    # cost로 H(x) 계산, W 업데이트\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch : {:4d}/{} Cost : {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7648e-04],\n",
      "        [3.1608e-02],\n",
      "        [3.8977e-02],\n",
      "        [9.5622e-01],\n",
      "        [9.9823e-01]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "print(hypothesis[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True]])\n"
     ]
    }
   ],
   "source": [
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True]])\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = prediction.float() == y_train\n",
    "print(correct_prediction[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)  # W와 b가 들어있는 Linear Layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    0/1000 Cost : 0.539713 Accurcay : 83.33%\n",
      "Epoch :  100/1000 Cost : 0.134272 Accurcay : 100.00%\n",
      "Epoch :  200/1000 Cost : 0.080486 Accurcay : 100.00%\n",
      "Epoch :  300/1000 Cost : 0.057820 Accurcay : 100.00%\n",
      "Epoch :  400/1000 Cost : 0.045251 Accurcay : 100.00%\n",
      "Epoch :  500/1000 Cost : 0.037228 Accurcay : 100.00%\n",
      "Epoch :  600/1000 Cost : 0.031649 Accurcay : 100.00%\n",
      "Epoch :  700/1000 Cost : 0.027538 Accurcay : 100.00%\n",
      "Epoch :  800/1000 Cost : 0.024381 Accurcay : 100.00%\n",
      "Epoch :  900/1000 Cost : 0.021877 Accurcay : 100.00%\n",
      "Epoch : 1000/1000 Cost : 0.019843 Accurcay : 100.00%\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)    \n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    # cost로 H(x) 계산, W 업데이트\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        accuarcy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        \n",
    "        print('Epoch : {:4d}/{} Cost : {:.6f} Accurcay : {:2.2f}%'.format(epoch, nb_epochs, cost.item(), accuarcy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
