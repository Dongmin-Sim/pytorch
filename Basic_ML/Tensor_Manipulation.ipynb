{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "edwith의 부스트 코스 : \"파이토치로 시작하는 딥러닝 기초\" 를 바탕으로 작성되었습니다.  \n",
    "https://www.boostcourse.org/ai214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basic Tensor Manipulation\n",
    "- Vector, Matrix and Tensor\n",
    "- Numpy Review\n",
    "- PyTorch Tensor Allocation\n",
    "- Matrix Multiplication\n",
    "- Other Basic Ops\n",
    "\n",
    "## 배울 내용\n",
    "1. 벡터, 매트릭스, 텐서에 대해 살펴보기\n",
    "2. numpy (파이토치가 넘파이와 굉장히 유사)\n",
    "3. 텐서를 어떻게 선언하는지, 선언된 텐서를 다루는 법\n",
    "4. 매트릭스 다루는 법\n",
    "4. 기타 오퍼레이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector, Matrix and Tensor\n",
    "\n",
    "기본적으로 차원이 없는 값은 **스칼라** 라고 부름.  \n",
    "1차원일 경우엔 **벡터**,  \n",
    "2차원일 경우에는 **행렬**, 영어로 **Matrix** 라고 함.   \n",
    "3차원일 경우에는 **텐서** 라고함.  \n",
    "\n",
    "우리가 사는 세상은 3차원이기 때문에 4차원부터는 표현과, 이해가 어려울 수 있음.  \n",
    "하지만 원리는 똑같기 때문에 3차원의 텐서를 위로 쌓아 올린 것이 4차원, 이를 옆으로 확장한 것이 5차원, 5차원을 깊이의 개념에서 뒤로 확장한 것을 6차원...  \n",
    "이런식으로 이해가 가능함.  \n",
    "\n",
    "앞으로 다룰 Matrix, Tensor의 크기를 계산하는 것이 굉장히 중요함.  \n",
    "매 연산 마다 어떠한 텐서, 매트릭스의 크기를 생각하고 있어야, 제대로된 구현이 가능하고, 새로운 수식이 주어졌을때고, 쉽게 이해가 가능함.  \n",
    "\n",
    "\n",
    "\n",
    "* 2D Tensor(Typical Simple Setting)\n",
    "2차원의 텐서가 주어졌을때, 이를 수식으로 나타낸다면, 아래와 같이 나타낼 수 있을 것  \n",
    "= batch_size * dimension 의 사이즈를 가진 Tensor t  \n",
    "예를 들어 batch_size = 64 , dim = 256 과 같은 64x256의 입력을 받아서 64*?? 과 같은 값의 매트릭스로 다시 뱉어주는 형태를 많이 보게 될것임.  \n",
    "\n",
    "$$\\vert t \\vert = (\\text{batch size, dim})$$\n",
    "\n",
    "* 3D Tensor(Typical Compute Vision)\n",
    "3차원의 텐서가 주어졌을때, 이를 수식으로 나타낸다면, 아래와 같이 나타낼 수 있을 것  \n",
    "= bathch_size * width * height 의 사이즈를 가진 Tensor t  \n",
    "주로 컴퓨터 비전 같은 경우에서는 이러한 3차원 텐서의 입력과 출력을 많이 다루게 되는데, 하나의 이미지에 대한 가로길이와 세로길이, 그리고 여러장의 이미지를 뜻하는 batch_size 를 구성하게 됨.  \n",
    "따라서 여러장의 이미지를 구성하게 되면 3차원의 텐서를 이루게 됨.  \n",
    "\n",
    "$$\\vert t \\vert = (\\text{batch size, width, height})$$\n",
    "\n",
    "* 3D Tensor(Typical Natural Language Processing)\n",
    "이러한 3차원의 텐서는 비전 뿐만 아니라 시계열, 시퀀셜 데이터, NLP(순서정보가 있는)의 경우에도 3차원 텐서를 가지고 입출력을 다룰 수 있음.\n",
    "\n",
    "$$\\vert t \\vert = (\\text{batcg size, length, dim})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Review\n",
    "numpy 와 PyTorch 는 굉장히 호환성이 높고, 직관적인 형태를 따르고 있음.\n",
    "넘파이와 파이토치 import 하는 방법은 다음과 같음\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Array with Numpy\n",
    "1차원의 array 를 선언하는 방법은 다음과 같음.  \n",
    "0~6까지의 수를 리스트의 형태로 `np.array`함수에 넣으면 1D 벡터로 선언이 가능함.  \n",
    "\n",
    "* 차원의 수를 확인 : `.ndim`\n",
    "* shape 를 확인 : `.shape`\n",
    "\n",
    "numpy 같은 경우, 파이썬과 마찬가지로 배열 리스트의 element에 대해서 접근과 슬라이싱이 쉬움.  \n",
    "고로 numpy의 1D array도 파이썬의 리스트와 같이 요소의 접근과, 슬라이싱이 가능함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of t:  1\n",
      "Shape of t:  (7,)\n"
     ]
    }
   ],
   "source": [
    "# 몇개의 차원으로 이루어졌는지 확인하는 함수 ndim\n",
    "print('Rank of t: ', t.ndim)\n",
    "\n",
    "# shape 를 확인\n",
    "print('Shape of t: ', t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수 t에는 하나의 차원에 대해서 7개의 element 를 가지는 1D벡터가 선언되어 있음을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t[0], t[1], t[-1] =  0.0 1.0 6.0\n",
      "t[2:5], t[4:-1] =  [2. 3. 4.] [4. 5.]\n",
      "t[:2] t[3:] = [0. 1.] [3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "print('t[0], t[1], t[-1] = ', t[0], t[1], t[-1])  # element\n",
    "print('t[2:5], t[4:-1] = ', t[2:5], t[4:-1])  # Slicing\n",
    "print('t[:2] t[3:] =', t[:2], t[3:])  # Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬의 리스트와 마찬가지로 [] 안의 인덱스값을 입력함으로써 element 에 접근하고, :를 이용하여 원하는 인덱스의 범위를 슬라이싱할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Array with Numpy\n",
    "2차원의 행렬 매트릭스에 대해서도 똑같이 할 수 있는 것을 볼 수 있음.  \n",
    "기본적으로 파이썬의 2차원 리스트를 선언하고, 1D 와 동일하게 `np.array` 함수에 넣어 2차원 배열을 선언함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]\n",
      " [10. 11. 12.]]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of t:  2\n",
      "Shape of t:  (4, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Rank of t: ', t.ndim)\n",
    "print('Shape of t: ', t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensor\n",
    "### 1D Array with PyTorch\n",
    "파이토치로 1D array 텐서를 선언할 수 있음. 앞선 numpy와 거의 비슷한 방식으로 가능함  \n",
    "* 차원의 수를 확인 : `.dim()`\n",
    "* 텐서의 모양을 확인 : `.shape`, `.size()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "tensor(0.) tensor(1.) tensor(6.)\n",
      "tensor([2., 3., 4.]) tensor([4., 5.])\n",
      "tensor([0., 1.]) tensor([3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim())  # rank\n",
    "print(t.shape)  # shape\n",
    "print(t.size())  # shape\n",
    "print(t[0], t[1], t[-1])  # Element\n",
    "print(t[2:5], t[4:-1])  # Slicing\n",
    "print(t[:2], t[3:])  # Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Array with PyTorch\n",
    "2차원의 경우에도 이전의 numpy에서 했던 것과 매우 비슷함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]\n",
    "                      ])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n",
      "tensor([ 2.,  5.,  8., 11.])\n",
      "torch.Size([4])\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 4.,  5.],\n",
      "        [ 7.,  8.],\n",
      "        [10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim())  # rank\n",
    "print(t.shape)  # shape\n",
    "print(t.size())\n",
    "print(t[:, 1])\n",
    "print(t[:, 1].size())\n",
    "print(t[:, :-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차원에 대해서도 마찬가지로 슬라이싱이 가능하며, 슬라이싱을 통해 가져온 벡터의 사이즈도 확인이 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tensor Manipulation\n",
    "### Broadcasting\n",
    "파이토치에서는 브로드 캐스팅이라는 기능을 제공함.   \n",
    "기본적으로 매트릭스 간의 연산을 수행할 때는 다음과 같은 법칙을 따라야 하는데 \n",
    "\n",
    "* 덧셈, 뺄셈을 할 때, 두 행렬간의 크기가 같아야 한다. \n",
    "* 두 행렬의 곱을 수행할 때는 행렬(1번째)의 마지막 차원과 행렬(2번째)첫번째 차원이 일치해야한다.\n",
    "\n",
    "이때 파이토치의 브로드캐스팅이라는 기능은 다른 크기를 갖는 행렬, 텐서에 대해 자동적으로 size 를 맞추어 연산을 수행해줌. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1 size:, torch.Size([1, 2]) \n",
      "m2 size : torch.Size([1, 2])\n",
      "\n",
      "m1 + m2 : tensor([[5., 5.]])\n",
      "m1 + m2 size : torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# same shape\n",
    "m1 = torch.FloatTensor([[3, 3]])\n",
    "m2 = torch.FloatTensor([[2, 2]])\n",
    "print('m1 size:,', m1.size(), '\\nm2 size :', m2.size())\n",
    "print('\\nm1 + m2 :', m1 + m2)\n",
    "print('m1 + m2 size :', (m1 + m2).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1 size:, torch.Size([1, 2]) \n",
      "m2 size : torch.Size([1])\n",
      "\n",
      "m1 + m2 : tensor([[5., 5.]])\n",
      "m1 + m2 size : torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# vector + scalar\n",
    "m1 = torch.FloatTensor([[3, 3]])\n",
    "m2 = torch.FloatTensor([2])\n",
    "print('m1 size:,', m1.size(), '\\nm2 size :', m2.size())\n",
    "print('\\nm1 + m2 :', m1 + m2)\n",
    "print('m1 + m2 size :', (m1 + m2).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "백터와 스칼라를 더하는 것도 브로드캐스팅 기능을 통해서 자동적으로 연산이 가능함.  \n",
    "기본적으로는 두 차원이 같지 않아 행렬연산이 불가능하지만, 파이토치가 이를 자동으로 변환해주어서 쉽게 연산이 가능함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1 size:, torch.Size([1, 2]) \n",
      "m2 size : torch.Size([2, 1])\n",
      "\n",
      "m1 + m2 :\n",
      " tensor([[4., 4.],\n",
      "        [5., 5.]])\n",
      "m1 + m2 size : torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 2*1 vector + 1*2 vector\n",
    "# vector + scalar\n",
    "m1 = torch.FloatTensor([[3, 3]])\n",
    "m2 = torch.FloatTensor([[1],\n",
    "                        [2]\n",
    "                       ])\n",
    "print('m1 size:,', m1.size(), '\\nm2 size :', m2.size())\n",
    "print('\\nm1 + m2 :\\n', m1 + m2)\n",
    "print('m1 + m2 size :', (m1 + m2).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서간의 연산에도 마찬가지로 수행을 하게 되는데 어떤 차원이 1만 들어있는 경우  \n",
    "자동으로 파이토치가 1이 들어있는 차원을 늘려줘서 연산이 진행됨.  \n",
    "위의 예시의 경우 두 텐서를 각각 [2, 2] 텐서로 만든 후 연산을 진행해준 것을 확인할 수 있음. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매우 편리한 기능이지만, 브로드캐스팅의 경우는 자동으로 실행되기 때문에 사용자 입장에서 굉장히 주의해서 사용을 해야함.    \n",
    "실수로 두 매트릭스, 텐서의 사이즈가 같게 계산을 해야했었는데, 다르게 사이즈가 입력되어도 파이토치는 이를 정상적으로 실행하기 때문에  \n",
    "에러가 발생하지 않아 이후 어디서 문제가 발생했는지 찾기가 어려울 수 있음.  그래서 항상 주의하면 사용해야함. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬 요소간 곱(기존 broadcasting) vs 행렬 곱(inner product, dot product)\n",
    "\n",
    "딥러닝은 행렬 곱 연산을 굉장히 많이 사용하는 알고리즘, 그래서 이 행렬 곱을 잘 이해하고 구현하는 것이 중요.  \n",
    "\n",
    "<행렬 연산>  \n",
    "이전에 살펴본 broadcating 으로 행렬간 차원을 맞추어 계산하는 것을 행렬 요소간 곱을 한다고 함.\n",
    "그러나 행렬 곱의 경우에는 계산방식이 다른데, 이를 inner product, dot product, 행렬 곱이라고 함.   \n",
    "앞서 언급한 \"행렬(1번째)의 마지막 차원과 행렬(2번째)첫번째 차원이 일치해야한다.\" 라는 조건을 만족해야지 곱셈이 이루어짐. \n",
    "\n",
    "행렬 요소간 곱은 `matrix1.mul(matrix2)` 으로 계산할 수 있고,  \n",
    "행렬 곱은 `matrix1.matmul(matrix2)` 으로 계산할 수 있음. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Mul(element wise) vs Matmul(inner product)\n",
      "---------------\n",
      "shape of m1 : torch.Size([2, 2])\n",
      "shape of m2 : torch.Size([2, 1])\n",
      "\n",
      " Result of Mul\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "\n",
      " Result of Matmul\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n"
     ]
    }
   ],
   "source": [
    "print('---------------')\n",
    "print('Mul(element wise) vs Matmul(inner product)')\n",
    "print('---------------')\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "\n",
    "print('shape of m1 :', m1.size())\n",
    "print('shape of m2 :', m2.size())\n",
    "print('\\n Result of Mul')\n",
    "print(m1.mul(m2))\n",
    "print('\\n Result of Matmul')\n",
    "print(m1.matmul(m2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Basic Ops\n",
    "### mean\n",
    "파이토치에는 평균을 구하는 방법도 제공하고 있음.   \n",
    "이도 numpy 와 매우 유사한데, dimension argument의 명칭만 다름.  \n",
    "LongTensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5000)\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([1, 2])\n",
    "print(t.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can only calculate the mean of floating types. Got Long instead.\n"
     ]
    }
   ],
   "source": [
    "t = torch.LongTensor([1, 2])\n",
    "try:\n",
    "    print(t.mean())\n",
    "except Exception as exc:\n",
    "    print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이보다 더 높은 차원의 텐서들도 `t.mean`을 통해서 모든 요소의 평균을 구할 수 있음.  \n",
    "특정한 차원에 대해서도 평균을 구할 수 있음. `mean()` 의 argument 로 `dim=` 을 주면 특정한 차원을 지정해줄 수 있음  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000)\n",
      "tensor([2., 3.])\n",
      "tensor([1.5000, 3.5000])\n",
      "\n",
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "print(t.mean())\n",
    "print(t.mean(dim=0))\n",
    "print(t.mean(dim=1))\n",
    "print()\n",
    "print(t.mean(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim=0 을 주게 되면 [2, 2] 의 메트릭스에서 0번째 차원을 줄여 [1, 2] 벡터로 출력됨. 열방향(같은 행끼리)으로 평균  \n",
    "dim=1 을 주게 되면 [2, 2] 의 매트릭스에서 1번째 차원을 줄여 [2, 1] 벡터로 출력됨. 행방향(같은 열끼리)으로 평균  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sum\n",
    "합을 구해주는 sum도 mean 과 동일함.  \n",
    "특정한 차원에 대해서도 합을 구할 수 있음. `sum()` 의 argument 로 `dim=` 을 주면 특정한 차원을 지정해줄 수 있음  \n",
    "인잣값이 주어지지 않으면 모든 요소에 대한 합을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor([4., 6.])\n",
      "tensor([3., 7.])\n",
      "\n",
      "tensor([3., 7.])\n"
     ]
    }
   ],
   "source": [
    "print(t.sum())\n",
    "print(t.sum(dim=0))\n",
    "print(t.sum(dim=1))\n",
    "print()\n",
    "print(t.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim=0 을 주게 되면 [2, 2] 의 메트릭스에서 0번째 차원을 줄여 [1, 2] 벡터로 출력됨. 열방향으로 합  \n",
    "dim=1 을 주게 되면 [2, 2] 의 매트릭스에서 1번째 차원을 줄여 [2, 1] 벡터로 출력됨. 행방향으로 합  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max, Argmax\n",
    "max : 어떠한 텐서나, 행렬에서 가장 큰 값을 리턴해주는 함수  \n",
    "argmax : 어떠한 텐서나, 행렬에서 가장 큰 값의 인덱스 값을 리턴해주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "print(t.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인잣값이 없으면 가장 큰 값을 리턴합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4.])\n",
      "tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim=0)[0])\n",
    "print(t.max(dim=0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 2개의 값을 return 하는데 하나는 해당 dimension 에서 가장 큰값들이고, 하나는 그에 해당하는 인덱스 값을 return 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim=1))\n",
    "print(t.max(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view\n",
    "view 는 numpy 에서 reshape 이라는 함수와 같은 역할을 함. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[[0, 1, 2],\n",
    "              [3, 4, 5]],\n",
    "              \n",
    "              [[6, 7, 8], \n",
    "              [9, 10, 11]]\n",
    "             ])\n",
    "\n",
    "ft = torch.FloatTensor(t)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1, 3]))\n",
    "print(ft.view([-1, 3]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2, 2, 3 -> 4, 3 으로 바꾸어준 모습임.   \n",
    "2 * 2 * 3 텐서 -> 4 * 3 행렬로 변환된 것\n",
    "\n",
    "-1 을 주면 다른 입력된 차원의 값에 맞게 자동으로 변환해줌.  \n",
    "그래서 -1은 보통 가장 변동이 심한 batch_size 나 기타 사용자가 사용하고 싶은 곳에 주로 사용됨.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1, 1, 3]))\n",
    "print(ft.view([-1, 1, 3]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view 함수를 사용해서 원하는 형태로 텐서를 자유자재로 모양을 바꿔줄 수 있음.  \n",
    "view 함수는 중요함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### squeeze\n",
    "view 함수를 사용한 것과 같으나, Squeeze에서는 자동으로 내가 원하는 dim 또는 전체에서 하나만 남아있는 경우, 1인 경우 없애주는 역할을 함.   \n",
    "해당 dim의 element의 개수가 1인 경우 없애주는 역할을 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.squeeze())\n",
    "print(ft.squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# dim = 0\n",
    "print(ft.squeeze(dim=0))\n",
    "print(ft.squeeze(dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# dim = 1\n",
    "print(ft.squeeze(dim=1))\n",
    "print(ft.squeeze(dim=1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인잣값으로 dim 을 주게 되면 해당 dim의 element 개수가 1일 경우만 squeeze를 해줌.  \n",
    "해당 차원의 요소의 개수가 1이 아니라면 변하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unsqueeze\n",
    "squeeze의 반대의 효과를 냄. 즉, 내가 원하는 dimension 에 1을 넣어줌  \n",
    "그렇기 때문에 dimension 을 꼭 명시해주어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([0, 1, 2])\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(0))\n",
    "print(ft.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view(1, -1))\n",
    "print(ft.view(1, -1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인잣값으로 차원의 숫자를 넣게 되면 0번째 차원에 1을 넣어달라고 하는 것과 같으며,  \n",
    "이는 view 로 (1, -1) 한 것과 같은 결과를 얻을 수 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(1))\n",
    "print(ft.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view(-1, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(-1))\n",
    "print(ft.unsqueeze(-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "lt = torch.LongTensor([1, 2, 3, 4])\n",
    "print(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "print(lt.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 1], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "bt = torch.ByteTensor([True, False, False, True])\n",
    "print(bt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ByterTensor로 True, False 의 값을 0, 1로 변환이 가능하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 1])\n",
      "tensor([1., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(bt.long())\n",
    "print(bt.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`long()`, `float()` 로 정수나 실수형으로 변환이 가능하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate\n",
    "이어 붙인는 뜻, 두 사이즈가 같은 텐서를 이어 붚여주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2]) torch.Size([2, 2])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "y = torch.FloatTensor([[5, 6], [7, 8]])\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "concat = torch.cat([x, y], dim=0)\n",
    "print(concat)\n",
    "print(concat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim의 값으로 0을 주게 되면 0번째 차원이 늘어나는 방식으로 concat 됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "concat = torch.cat([x, y], dim=1)\n",
    "print(concat)\n",
    "print(concat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking\n",
    "concatenation 을 좀 더 편리하게 이용하게 해주는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([1, 4])\n",
    "y = torch.FloatTensor([2, 5])\n",
    "z = torch.FloatTensor([3, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack([x, y, z]))\n",
    "print(torch.stack([x, y, z], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 4., 2., 5., 3., 6.])\n",
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat([x, y, z], dim=0))\n",
    "print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ones and zeros\n",
    "입력값의 shape과 같은 shape의 1, 0으로 가득찬 텐서를 반환함.  \n",
    "<참고>\n",
    "같은 디바이스에는 같은 텐서를 사용해야 한다. GPU 텐서와 CPU 텐서간의 연산을 수행하게 되면 에러가 나게 됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [2., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones_like(x))\n",
    "print(torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place Operation\n",
    "_ 언더스코어가 붙어있으며, in_place 기존의 텐서를 바꿔주는 역할을 함.\n",
    "in place = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.mul(2.))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.mul_(2.))\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
