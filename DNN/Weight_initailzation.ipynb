{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "edwith의 부스트 코스 : \"파이토치로 시작하는 딥러닝 기초\" 를 바탕으로 작성되었습니다.  \n",
    "https://www.boostcourse.org/ai214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight initialization\n",
    "\n",
    "- Why good initialization?\n",
    "- RBM / DBN\n",
    "- Xavier / He initialization\n",
    "- Full code : mnist_nn_xavier\n",
    "- Full code : mnist_nn_deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why good initialization?\n",
    "Need to set the initial weight values wisely  \n",
    "* Not all 0's (neural network 를 업데이트할 때, backpropagation 알고리즘을 사용하는데 gradient를 계산을 해서 weight를 업데이트하는데 weight가 0이게 되면 모든 gradient값이 0으로 바뀌게 됨. - 학습이 불가능)\n",
    "* Challenging issue\n",
    "* Hinton et al. (2006) \" A Fast Learning Algorithm for Deep Belief Nets\" - Restricted Boltzmann Machine (RBM)을 이용해서 weight initialization을 하게 되면 성능이 올라간다는 논문\n",
    "\n",
    "실제로 initialization 을 무작위로 설정하는 것 보다 weight initialization 방식을 적용한 모델들이 더 성능이 좋았다는 연구결과가 존재   \n",
    "    $\\therefore$ weight initialization 을 잘 하는 것이 딥러닝의 성능에 큰 영향을 미치는 것을 확인할 수 있음. . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM / DBN\n",
    "* Restricted Boltzmann Machine\n",
    "    * Restricted : no connections within a layer\n",
    "    * KL divergence : compare actual to recreation\n",
    "    \n",
    "*How we can use RBM to initialize weights?*\n",
    " * Apply the RBM idea on adjacent two layers as a pre-training step\n",
    " * Continue the first process to all layers\n",
    " * This will set weights \n",
    " * Example : Deep Belief Network\n",
    "     * Weight initialized by RBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier / He initialization\n",
    "* No need to use complicated RBM for weight initializations \n",
    "* Simple methods are OK\n",
    "    * **Xavier initialization** : X. Glorot and Y.Bengio, \"Understanding the difficulty of training deep feedforward neural networks.\" in International conference on artificial intelligence and statistics, 2010\n",
    "    * **He initialization** : K.He, X.Zang, S.Ren, and J.Sun, \"Delving Deep into Rectifiers: Suprpassing Human-Level Performance on ImangeNet Classification\", 2015\n",
    "\n",
    "무작위적으로 weights를 initialized 하는 것이 아니라, layer에 특성에 따라서 다르게 initialization 을 해야한다.\n",
    "\n",
    "### Xavier\n",
    "RBM 처럼 pre-training step 을 거치거나 하는 복잡한 형태가 아닌 수식으로써 간단하게 weights 를 초기화 하는 방법 2가지가 존재함. \n",
    "* Xavier Normal initialization\n",
    "$$W \\sim N(0, Var(W))$$\n",
    "$$Var(W) = \\sqrt{\\dfrac{2}{n_{in}+n_{out}}}$$\n",
    "\n",
    "n_in 은 레이어의 input 수를 의미하고, out은 레이어의 output의 수를 의미함. \n",
    "\n",
    "* Xavier Uniform initialization\n",
    "$$W \\sim U(-\\sqrt{\\dfrac{6}{n_{in}+n_{out}}}, + \\sqrt{\\dfrac{6}{n_{in}+n_{out}}})$$\n",
    "\n",
    "### He initialization\n",
    "Xavier initialization 의 변형, n_out term 없이 사용되는 초기화 방법\n",
    "\n",
    "* He Normal initialization\n",
    "$$W \\sim N(0, Var(W))$$\n",
    "$$Var(W) = \\sqrt{\\dfrac{2}{n_{in}}}$$\n",
    "\n",
    "n_in 은 레이어의 input 수를 의미하고, out은 레이어의 output의 수를 의미함. \n",
    "\n",
    "* He Uniform initialization\n",
    "$$W \\sim U(-\\sqrt{\\dfrac{6}{n_{in}}}, + \\sqrt{\\dfrac{6}{n_{in}}})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full code : mnist_nn_xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda': \n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "traing_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dsets.MNIST(root='../MNIST_data/',\n",
    "                         train=True,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=False)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='../MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(mnist_train,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn layer\n",
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0749,  0.1136,  0.0031,  ...,  0.0042,  0.1258,  0.0327],\n",
       "        [ 0.0348,  0.0790, -0.1096,  ..., -0.0396, -0.0324,  0.1384],\n",
       "        [ 0.1407,  0.1386, -0.0059,  ...,  0.0106, -0.0942, -0.0712],\n",
       "        ...,\n",
       "        [-0.0367,  0.1250,  0.0028,  ...,  0.0386, -0.0834, -0.1472],\n",
       "        [ 0.0443,  0.0738, -0.1421,  ...,  0.1075, -0.0641,  0.0136],\n",
       "        [ 0.1313,  0.0921, -0.0932,  ..., -0.0239, -0.0482, -0.1415]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xavier_uniform initialization\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "total_batch = len(data_loader)\n",
    "for epoch in range(traing_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        x_train = X.view(-1, 28 * 28).to(device)\n",
    "        y_train = Y.to(device)\n",
    "        \n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        cost = criterion(prediction, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / total_batch\n",
    "    \n",
    "    print('Epoch : {:4d}/{}, Cost : {:.6f}'.format(epoch, traing_epochs, cost.item()))\n",
    "    \n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "with torch.no_grad():\n",
    "    x_test = mnist_test.data.view(-1, 28*28).float().to(device)\n",
    "    y_test = mnist_test.target.float().to(device)\n",
    "    \n",
    "    pred = model(x_test)\n",
    "    correct = torch.argmax(pred, 1) == y_test\n",
    "    \n",
    "    accuracy = correct.float().mean()\n",
    "    print('Accuracy : {:5f}%'.format(accuracy * 100))\n",
    "    \n",
    "    # get random one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    \n",
    "    x_single = mnist_test.data[r].view(-1, 28 * 28).float().to(device)\n",
    "    y_single = mnist_test.target[r].float().to(device)\n",
    "    \n",
    "    pred = model(x_single)\n",
    "    \n",
    "    print('Label : ', y_single.item())\n",
    "    print('Prediction', torch.argmax(pred, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full code : mnist_nn_deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn layer\n",
    "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
    "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
    "relu = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0714, -0.0682,  0.0077,  ...,  0.0413,  0.0087, -0.0673],\n",
       "        [-0.0304, -0.0842, -0.0350,  ..., -0.0041, -0.0595,  0.0582],\n",
       "        [ 0.0194,  0.0091, -0.0018,  ..., -0.0852,  0.0177,  0.0429],\n",
       "        ...,\n",
       "        [-0.0685,  0.0552,  0.0257,  ...,  0.0862, -0.1019, -0.0520],\n",
       "        [ 0.0563,  0.0515,  0.0608,  ...,  0.0279,  0.0008,  0.0822],\n",
       "        [-0.0808,  0.0378,  0.0452,  ..., -0.0638,  0.0668, -0.0434]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xavier_uniform initialization\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "torch.nn.init.xavier_uniform_(linear4.weight)\n",
    "torch.nn.init.xavier_uniform_(linear5.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3, relu, linear4, relu, linear5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch = len(data_loader)\n",
    "for epoch in range(traing_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        x_train = X.view(-1, 28 * 28).to(device)\n",
    "        y_train = Y.to(device)\n",
    "        \n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        cost = criterion(prediction, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / total_batch\n",
    "    \n",
    "    print('Epoch : {:4d}/{}, Cost : {:.6f}'.format(epoch, traing_epochs, cost.item()))\n",
    "    \n",
    "print('Training finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
