{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "edwith의 부스트 코스 : \"파이토치로 시작하는 딥러닝 기초\" 를 바탕으로 작성되었습니다.  \n",
    "https://www.boostcourse.org/ai214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "- Perceptron\n",
    "- AND, OR\n",
    "- XOR\n",
    "- Full code : XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "인공신경망의 한 종류. 뉴런의 동작방식을 본 뜬 모델을 인공신경망 이라고 함.  \n",
    "Perceptron 은 어떠한 입력 X가 들어왔을때 이 X 에 weight, 가중치를 곱하게 되고, bias 를 더해 ouput 을 만들어 냄. output 은 다시한번 activation function 을 거쳐 최종적으로 output 이 산출됨.  \n",
    "여기서 말하는 activation function 은 sigmoid 와 같은 function 을 말함. 초창기의 perception 은 linear classifier 을 위해 만들어짐.  \n",
    "\n",
    "1950년도 perceptron 이 개발되었는데 AND, OR 문제를 해결하기 위해 고안됨.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AND, OR\n",
    "두 가지 입력 A,B에 대하여\n",
    "\n",
    "* AND 게이트\n",
    "    * (0, 0) -> 0 \n",
    "    * (0, 1) -> 0\n",
    "    * (1, 0) -> 0\n",
    "    * (1, 1) -> 1\n",
    "\n",
    "* OR 게이트\n",
    "    * (0, 0) -> 0 \n",
    "    * (0, 1) -> 1\n",
    "    * (1, 0) -> 1\n",
    "    * (1, 1) -> 1\n",
    "\n",
    "와 같은 결과로 동작하는 AND, OR 게이트를 의미함. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR\n",
    "\n",
    "민스키 가 1개의 layer 를 갖는 Perceptron 이라는 구조는 XOR 문제를 해결할 수 없다라는 것을 증명함.  \n",
    "multilayer 를 쌓아야만 XOR 문제를 해결할 수 있다. 하지만, 각 층의 여러개의 층의 weight 를 학습시킬 수 있는 방법이 존재하지 않는 다는 것을 증명함과 동시에 인공신경망 분야의 암흑기가 옴.  \n",
    "현재는 이 multilayer를 학습할 수 있는 back propagation 이라는 방법이 고안되어서 이 문제를 해결할 수 있었지만, 당시에는 이를 해결할 방법이 존재하지 않아 neural network 가 암흑기에 빠지게 됨.  \n",
    "\n",
    "* XOR 게이트\n",
    "    * (0, 0) -> 0 \n",
    "    * (0, 1) -> 1\n",
    "    * (1, 0) -> 1\n",
    "    * (1, 1) -> 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full code : XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0, cost : 0.703402042388916\n",
      "step : 100, cost : 0.6931477189064026\n",
      "step : 200, cost : 0.6931471824645996\n",
      "step : 300, cost : 0.6931471824645996\n",
      "step : 400, cost : 0.6931471824645996\n",
      "step : 500, cost : 0.6931471824645996\n",
      "step : 600, cost : 0.6931471824645996\n",
      "step : 700, cost : 0.6931471824645996\n",
      "step : 800, cost : 0.6931471824645996\n",
      "step : 900, cost : 0.6931471824645996\n",
      "step : 1000, cost : 0.6931471824645996\n",
      "step : 1100, cost : 0.6931471824645996\n",
      "step : 1200, cost : 0.6931471824645996\n",
      "step : 1300, cost : 0.6931471824645996\n",
      "step : 1400, cost : 0.6931471824645996\n",
      "step : 1500, cost : 0.6931471824645996\n",
      "step : 1600, cost : 0.6931471824645996\n",
      "step : 1700, cost : 0.6931471824645996\n",
      "step : 1800, cost : 0.6931471824645996\n",
      "step : 1900, cost : 0.6931471824645996\n",
      "step : 2000, cost : 0.6931471824645996\n",
      "step : 2100, cost : 0.6931471824645996\n",
      "step : 2200, cost : 0.6931471824645996\n",
      "step : 2300, cost : 0.6931471824645996\n",
      "step : 2400, cost : 0.6931471824645996\n",
      "step : 2500, cost : 0.6931471824645996\n",
      "step : 2600, cost : 0.6931471824645996\n",
      "step : 2700, cost : 0.6931471824645996\n",
      "step : 2800, cost : 0.6931471824645996\n",
      "step : 2900, cost : 0.6931471824645996\n",
      "step : 3000, cost : 0.6931471824645996\n",
      "step : 3100, cost : 0.6931471824645996\n",
      "step : 3200, cost : 0.6931471824645996\n",
      "step : 3300, cost : 0.6931471824645996\n",
      "step : 3400, cost : 0.6931471824645996\n",
      "step : 3500, cost : 0.6931471824645996\n",
      "step : 3600, cost : 0.6931471824645996\n",
      "step : 3700, cost : 0.6931471824645996\n",
      "step : 3800, cost : 0.6931471824645996\n",
      "step : 3900, cost : 0.6931471824645996\n",
      "step : 4000, cost : 0.6931471824645996\n",
      "step : 4100, cost : 0.6931471824645996\n",
      "step : 4200, cost : 0.6931471824645996\n",
      "step : 4300, cost : 0.6931471824645996\n",
      "step : 4400, cost : 0.6931471824645996\n",
      "step : 4500, cost : 0.6931471824645996\n",
      "step : 4600, cost : 0.6931471824645996\n",
      "step : 4700, cost : 0.6931471824645996\n",
      "step : 4800, cost : 0.6931471824645996\n",
      "step : 4900, cost : 0.6931471824645996\n",
      "step : 5000, cost : 0.6931471824645996\n",
      "step : 5100, cost : 0.6931471824645996\n",
      "step : 5200, cost : 0.6931471824645996\n",
      "step : 5300, cost : 0.6931471824645996\n",
      "step : 5400, cost : 0.6931471824645996\n",
      "step : 5500, cost : 0.6931471824645996\n",
      "step : 5600, cost : 0.6931471824645996\n",
      "step : 5700, cost : 0.6931471824645996\n",
      "step : 5800, cost : 0.6931471824645996\n",
      "step : 5900, cost : 0.6931471824645996\n",
      "step : 6000, cost : 0.6931471824645996\n",
      "step : 6100, cost : 0.6931471824645996\n",
      "step : 6200, cost : 0.6931471824645996\n",
      "step : 6300, cost : 0.6931471824645996\n",
      "step : 6400, cost : 0.6931471824645996\n",
      "step : 6500, cost : 0.6931471824645996\n",
      "step : 6600, cost : 0.6931471824645996\n",
      "step : 6700, cost : 0.6931471824645996\n",
      "step : 6800, cost : 0.6931471824645996\n",
      "step : 6900, cost : 0.6931471824645996\n",
      "step : 7000, cost : 0.6931471824645996\n",
      "step : 7100, cost : 0.6931471824645996\n",
      "step : 7200, cost : 0.6931471824645996\n",
      "step : 7300, cost : 0.6931471824645996\n",
      "step : 7400, cost : 0.6931471824645996\n",
      "step : 7500, cost : 0.6931471824645996\n",
      "step : 7600, cost : 0.6931471824645996\n",
      "step : 7700, cost : 0.6931471824645996\n",
      "step : 7800, cost : 0.6931471824645996\n",
      "step : 7900, cost : 0.6931471824645996\n",
      "step : 8000, cost : 0.6931471824645996\n",
      "step : 8100, cost : 0.6931471824645996\n",
      "step : 8200, cost : 0.6931471824645996\n",
      "step : 8300, cost : 0.6931471824645996\n",
      "step : 8400, cost : 0.6931471824645996\n",
      "step : 8500, cost : 0.6931471824645996\n",
      "step : 8600, cost : 0.6931471824645996\n",
      "step : 8700, cost : 0.6931471824645996\n",
      "step : 8800, cost : 0.6931471824645996\n",
      "step : 8900, cost : 0.6931471824645996\n",
      "step : 9000, cost : 0.6931471824645996\n",
      "step : 9100, cost : 0.6931471824645996\n",
      "step : 9200, cost : 0.6931471824645996\n",
      "step : 9300, cost : 0.6931471824645996\n",
      "step : 9400, cost : 0.6931471824645996\n",
      "step : 9500, cost : 0.6931471824645996\n",
      "step : 9600, cost : 0.6931471824645996\n",
      "step : 9700, cost : 0.6931471824645996\n",
      "step : 9800, cost : 0.6931471824645996\n",
      "step : 9900, cost : 0.6931471824645996\n",
      "step : 10000, cost : 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# nn layers\n",
    "linear = nn.Linear(2, 1, bias=True)\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "model = nn.Sequential(linear, sigmoid).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for step in range(10001):\n",
    "    hypothesis = model(X)\n",
    "    \n",
    "    cost = torch.nn.functional.binary_cross_entropy(hypothesis, Y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    if step % 100 == 0:\n",
    "        print('step : {}, cost : {}'.format(step, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss 값이 전혀 줄어들지 않고 있음. 학습이 제대로 이루어지지 않고 있다는 것을 뜻하고, 실제로 데이터를 넣어봤을때도, 모두 0.5로 예측하는 모습을 볼 수 있음.  \n",
    "즉, 정확도가 0.5(50%)에서 더 향상되지 않는 것을 확인할 수 있음.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
